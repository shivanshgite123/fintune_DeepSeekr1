# ğŸš€ Deepseek LLM Fine-Tuning with LoRA

> **Fine-tune the Deepseek 7B Chat model using LoRA (PEFT) on the Alpaca dataset**

---

## ğŸ“‹ Quick Links
- [ğŸ¯ Overview](#overview)
- [âš™ï¸ Prerequisites](#prerequisites)
- [ğŸ“¦ Installation](#installation)
- [ğŸƒ Quick Start](#quick-start)
- [ğŸ’¾ Load Model](#load-model)
- [ğŸ› Troubleshooting](#troubleshooting)

---

## ğŸ¯ Overview

This is a complete fine-tuning pipeline for **Deepseek 7B Chat** using **LoRA (Low-Rank Adaptation)** on the **Alpaca dataset**.

### âœ¨ Features

âœ… Memory Efficient â€” LoRA only trains adapter weights  
âœ… Fast Training â€” FP16 precision + gradient accumulation  
âœ… Production Ready â€” Saves model and tokenizer  
âœ… Resume Support â€” Continue from checkpoints  
âœ… Chat Format â€” Instruction-response pairs  

---

## âš™ï¸ Prerequisites

### Hardware
- GPU VRAM: 20+ GB (RTX 4090, A100, etc.)
- CPU RAM: 16+ GB
- Disk Space: 50+ GB
- OS: Windows with PowerShell

### Software
- Python 3.8+
- Hugging Face account (optional)

---

## ğŸ“¦ Installation

### 1. Create Virtual Environment

```powershell
python -m venv deepseek_env
.\deepseek_env\Scripts\Activate.ps1
